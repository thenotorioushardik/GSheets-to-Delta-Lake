{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GoogleSheetsToDelta\").getOrCreate()\n",
    "\n",
    "# Service account credentials (Replace with actual values)\n",
    "service_account_info = {\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"YOUR_PROJECT_ID\",\n",
    "    \"private_key_id\": \"YOUR_PRIVATE_KEY_ID\",\n",
    "    \"private_key\": \"YOUR_PRIVATE_KEY\",\n",
    "    \"client_email\": \"YOUR_CLIENT_EMAIL\",\n",
    "    \"client_id\": \"YOUR_CLIENT_ID\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"YOUR_CLIENT_CERT_URL\",\n",
    "    \"universe_domain\": \"googleapis.com\"\n",
    "}\n",
    "\n",
    "# Google Sheets API authentication\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_dict(service_account_info, scope)\n",
    "client = gspread.authorize(credentials)\n",
    "\n",
    "# Google Sheets data mapping\n",
    "sheets_data = [\n",
    "    {\n",
    "        \"sheet_url\": \"YOUR_GOOGLE_SHEET_URL_1\",\n",
    "        \"worksheet_name\": \"YOUR_WORKSHEET_NAME_1\",\n",
    "        \"table_name\": \"YOUR_DELTA_TABLE_NAME_1\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_url\": \"YOUR_GOOGLE_SHEET_URL_2\",\n",
    "        \"worksheet_name\": \"YOUR_WORKSHEET_NAME_2\",\n",
    "        \"table_name\": \"YOUR_DELTA_TABLE_NAME_2\"\n",
    "    },\n",
    "    {\n",
    "        \"sheet_url\": \"YOUR_GOOGLE_SHEET_URL_3\",\n",
    "        \"worksheet_name\": \"YOUR_WORKSHEET_NAME_3\",\n",
    "        \"table_name\": \"YOUR_DELTA_TABLE_NAME_3\"\n",
    "    }\n",
    "    # Add more sheets here if needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename duplicate column names\n",
    "def rename_columns(columns):\n",
    "    counter = Counter()\n",
    "    new_columns = []\n",
    "    \n",
    "    for col in columns:\n",
    "        col = col.strip() or \"unnamed\"  # If empty, rename to 'unnamed'\n",
    "\n",
    "        if counter[col]:  # If duplicate, append .1, .2, etc.\n",
    "            new_col = f\"{col}.{counter[col]}\"\n",
    "        else:\n",
    "            new_col = col\n",
    "        \n",
    "        new_columns.append(new_col)\n",
    "        counter[col] += 1  # Increase count for next duplicate\n",
    "        \n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in sheets_data:\n",
    "    spreadsheet = client.open_by_url(item[\"sheet_url\"])\n",
    "    worksheet = spreadsheet.worksheet(item[\"worksheet_name\"])\n",
    "\n",
    "    data = worksheet.get_all_values()\n",
    "\n",
    "    pandas_df = pd.DataFrame(data)\n",
    "    pandas_df.columns = rename_columns(pandas_df.iloc[0])\n",
    "\n",
    "    # Drop the first row (since it's now the header)\n",
    "    pandas_df = pandas_df[1:].reset_index(drop=True)\n",
    "    pandas_df = pandas_df.astype(str)\n",
    "\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Drop existing table if it exists\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {item['table_name']}\")\n",
    "\n",
    "    # Save as Delta Table in Databricks\n",
    "    spark_df.write.format(\"delta\").option(\"delta.columnMapping.mode\", \"name\").mode(\"overwrite\").saveAsTable(item['table_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
